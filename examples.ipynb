{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T13:31:58.954518Z",
     "start_time": "2020-09-11T13:31:58.952259Z"
    }
   },
   "source": [
    "# PyTorch Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This example requires:\n",
    "\n",
    "- PyTorch 1.6+\n",
    "- torchaudio 0.7+\n",
    "- omegaconf (or any similar library to work with yaml files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:21:24.761414Z",
     "start_time": "2020-09-11T14:21:24.239616Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from glob import glob\n",
    "from omegaconf import OmegaConf\n",
    "from utils import (init_jit_model, \n",
    "                   split_into_batches,\n",
    "                   read_batch,\n",
    "                   prepare_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:21:25.234818Z",
     "start_time": "2020-09-11T14:21:25.218179Z"
    }
   },
   "outputs": [],
   "source": [
    "models = OmegaConf.load('models.yml')  # all available models are listed in the yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:21:26.056045Z",
     "start_time": "2020-09-11T14:21:26.040771Z"
    }
   },
   "outputs": [],
   "source": [
    "list(models.stt_models.keys()), list(models.stt_models.en.keys()), list(models.stt_models.en.latest.keys()), models.stt_models.en.latest.jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:25:14.996913Z",
     "start_time": "2020-09-11T14:21:40.831866Z"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cpu')   # you can use any pytorch device\n",
    "model, decoder = init_jit_model(models.stt_models.en.latest.jit, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T13:57:09.061692Z",
     "start_time": "2020-09-11T13:57:08.992493Z"
    }
   },
   "outputs": [],
   "source": [
    "test_files = glob('path/to/your/file/*.opus')\n",
    "batches = split_into_batches(test_files, batch_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:38:32.972790Z",
     "start_time": "2020-09-11T14:38:31.605231Z"
    }
   },
   "outputs": [],
   "source": [
    "input = prepare_model_input(read_batch(random.sample(batches, k=1)[0]),\n",
    "                            device=device)\n",
    "output = model(input)\n",
    "for example in output:\n",
    "    print(decoder(example.cpu()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ONNX example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:03:40.034192Z",
     "start_time": "2020-09-11T14:03:40.029634Z"
    }
   },
   "source": [
    "This example requires:\n",
    "\n",
    "- PyTorch 1.6+\n",
    "- torchaudio 0.7+\n",
    "- omegaconf (or any similar library to work with yaml files)\n",
    "- onnx\n",
    "- onnxruntime\n",
    "\n",
    "Differences:\n",
    "\n",
    "- ONNX runtime is used for inference;\n",
    "- I am reusing PyTorch utils, but you may use your own;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:14:23.911681Z",
     "start_time": "2020-09-11T14:14:23.149667Z"
    }
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import onnx\n",
    "import torch\n",
    "import random\n",
    "import tempfile\n",
    "import onnxruntime\n",
    "from glob import glob\n",
    "from omegaconf import OmegaConf\n",
    "from utils import (init_jit_model, Decoder, read_batch,\n",
    "                   split_into_batches, prepare_model_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:14:24.015836Z",
     "start_time": "2020-09-11T14:14:23.999060Z"
    }
   },
   "outputs": [],
   "source": [
    "models = OmegaConf.load('models.yml')  # all available models are listed in the yml file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:14:25.092585Z",
     "start_time": "2020-09-11T14:14:25.078048Z"
    }
   },
   "outputs": [],
   "source": [
    "list(models.stt_models.en.latest)  # see which models are available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:15:01.939254Z",
     "start_time": "2020-09-11T14:14:25.969658Z"
    }
   },
   "outputs": [],
   "source": [
    "with tempfile.NamedTemporaryFile('wb', suffix='.json') as f:\n",
    "    torch.hub.download_url_to_file(models.stt_models.en.latest.labels,\n",
    "                               f.name,\n",
    "                               progress=True)\n",
    "    with open(f.name) as f:\n",
    "        labels = json.load(f)\n",
    "        decoder = Decoder(labels)\n",
    "\n",
    "\n",
    "with tempfile.NamedTemporaryFile('wb', suffix='.model') as f:\n",
    "    torch.hub.download_url_to_file(models.stt_models.en.latest.onnx,\n",
    "                                   f.name,\n",
    "                                   progress=True)\n",
    "    onnx_model = onnx.load(f.name)\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    ort_session = onnxruntime.InferenceSession(f.name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:16:06.722402Z",
     "start_time": "2020-09-11T14:16:06.645751Z"
    }
   },
   "outputs": [],
   "source": [
    "# note that for now ONNX supports only batchless models, i.e. just samples\n",
    "# as it is mostly intended for porting the network elsewhere\n",
    "\n",
    "test_files = glob('path/to/your/file/*.opus')\n",
    "batches = split_into_batches(test_files, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:19:24.677767Z",
     "start_time": "2020-09-11T14:19:24.671730Z"
    }
   },
   "outputs": [],
   "source": [
    "input = prepare_model_input(\n",
    "    read_batch(\n",
    "        random.sample(batches, k=1)[0]\n",
    "    )\n",
    ").detach().cpu().numpy()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-11T14:19:25.395918Z",
     "start_time": "2020-09-11T14:19:25.312931Z"
    }
   },
   "outputs": [],
   "source": [
    "ort_inputs = {'input': input}\n",
    "ort_outs = ort_session.run(None, ort_inputs)\n",
    "decoded = decoder(torch.Tensor(ort_outs[0]))\n",
    "print(decoded)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
